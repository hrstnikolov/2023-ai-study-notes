# 05 Support Vector Machines

## Въведение

**Проект**
- обучаване на модели върху данни,
- как подаваме данни, как конфигурираме хиперпараметрите,
- как диагностицираме модела,
- как обясняваме тегловните коефициенти (на линейна регресия,  PCA),
- за класификация: residual plot, както на скалираните така и на първоначалния ИЗХОД
- как решаваме кой хиперпараметри да оптимизираме,
- смисъл на хиперпараметри
- какви метрики използваме,
- кой scaler да използвам
- feature extraction, PCA, на неструктурирани данни
- да показа classification_report, confusion_matrix,
- примери (от тестът)в които моделът бърка
- да използвам pipelines

обработка на човешка реч = *NLP*

saliency map

Когато имаме не-таблични данни, първо трябва да ги превърнем в таблични.

## SVM

### Интуиция

всички алгоритми дотук може да ползваме и за регресия и за класификация
дъветата са по-добри класификатори отколкото регресори

входни параметъра = променливи на фунцкията

Спирка с чакащи гледана огторе. Трябва да стигна от start (S) до end (E).
Как може да опитимизам пътеката така че да блъсна най-малко хора?

Нова картинка. Два класа. Искаме да ги отделим.
Може да направим Анова тест, F test, ще ни покаже че са разделени двете групи наблюдения.
Търся коя функиця е възможно най-далече от тях.

И двете линии имат 100% accuracy. Но някакси жълтата е по-далече от данните. Тук идва идеята на *опорните вектори*.

Support Vector Machines = *метод на опорните вектори*

по-логично би било да се наричат 'опорни точки', но ползваме 'вектори'

интересува ме само разстоянието до най-близките точки, него искам да оптимизирам
това разстояние ще наричаме *margin* - разстояние до най-близката точка от група

support vector са защото 'подпират' точките
points of the edge of the dividing hyperplane

уравнение на права

разделянето се нарича още *shattering*

*soft margin*

SVM
- algorithm
- tries to fit a line (= hyperplane) between the different classes to separate them
- the line shall be chosen so that the distance to the points of the classes is max 

###  Хиперпараметри

*C* - penalty for misclassification

SVM е непрактично бавен при данни с голям брой записи (напр > 1M)

Винаги на практика трябва да скалираме данните. Само в единични случай това влошава данните.w

**LinearSVC** е по-бърз, **SVC** може да прави повече неща

хиперпараметрите на SVC много приличат на тези на логистична регресия

Най-важният хиперпараметър (отново) е коеф на регуляризация.

при SVC/SVR имаме САМО L1 или L2 регуляризация, не може и двете

L1 - прави feature selection
L2 - намалява стойността на всички тегла заедно

SVM са математически доказано **универсални**, работят за всичко




### Пример: LinearSVC

имаме една група `.coef_` защото имаме 2 класа

тегловните коефициенти на класификатора (модела) можем да ползваме за интерпретация кои са важните feature-и. Това се нарича **feature importances**.

поредица от импорти от sklean:
1. preprocessing
2. модели
3. алгоритми за избор на модел
4. метрикки

метриката за оптимизиране в GridSearch по подразбиране е accuracy. Само че ние имаме  небалансирани класове (23% са с високи доходи), така че accuracy не е добра метрика. Вместо това, искаме да ползваме F1.

използваме `make_scorer` защото не може да извикаме `f1_score()`. Така извикано `f1_score()` е call функцията и ще иска y_true и y_pred.

Задаваме си и положителния label - този, който ни интересува, в случея - хората с високи доходи (които са по-малко записи).

```python
linear_grid_search = GridSearchCV(
    estimator=LinearSVC(max_iter = 1000),
    param_grid=param_grid,
    scoring = make_scorer(f1_score, pos_label=">50K"),
)
```


### Интерпретация на classification_report

Интерпретация на classification_report-а долу:
- '>50K' има сравнително голям precission и малък recall
	- голям precission = предсказанията са точни, където е казал че е '>50К' наистина е познал
	- малък recall = хванал е малко от хората с висок доход ('>50К'), пропуснал е по-голямата част
- ?? Моделът предсказва еднакво добре и двата сета следоватално **моделът е стабилен**. Няма голяма нестабилност, няма high variance.
	- Нестабилен модел (high variance, overfitting) би предсказвал много точно трениращите данни, но би се представял значително по-лошо на тестови.

![[Pasted image 20221013141413.png]]




### Kernal trick

Колкото повече измерения имат входните данни, толко по-вероятно е да има хипер-равнина, която ги разделя (**Cover's theorem**).

Превръщане на задача с 1 измерение на входните данни (1 feature) в задача с 2 измерения. Правим данните линейно разделими като добавяме повече измерения.

Идеята прилича на това да направим полиномна регресия от линейна като създадем допълнителни колони с произведения (между колони; polynomial features).

Взимаме данните и проектираме в нови измерения. Целта е новия сет вече да е **линейно разделим**. Функцията която прави преобразуването се нарича **kernal function**.

kernal при конволюции на изображения != kernal функция при SVM

radial basis function (**rbf**)
- функция, която е радиална
- едно гаусово разпределение в повече на брой измерения

kernel='poly' ще работи както работят полиномните feature-и




### Пример: SVC with polynomial kernel

- имам голям P на единия клас и голям процент A на другия. Това означава че моделът бърка голяма част от данните. Индикация е че моделът има нужда от повече итерации за да тренира.
- Нормално е да направим хубава настройка на хиперпараметрите (grid-search). Друга идея - да увеличим C (по-малко регуляризация).
- Алгоритъм, който не е научен, т.е. не е направил достатъчно стъпки за да достигне минимум, има голям bais.
![[Pasted image 20221013162218.png]]


различни kernels [link](https://en.wikipedia.org/wiki/Positive-definite_kernel)

Линейният kernal представлява скаларното произведение на 2 вектора.

триъгълните скоби (**angled brackets**) е друг за **скаларно произведение**  

### Интуиция rbf
- Имаме гаусов кернел, 3Д. Той прави 'шапка'. С параметърът $\gamma$ контролираме стандартното отклонение, т.е. колко е 'широка' шапката.
- Всяка точка от данните се проектира на повърхността на шапката. Всяка точка ПОД шапката ще е от единия клас, всяка точка ИЗВЪН - от другия.
- Тясна шапка -> напр само 1 точка под нея, всички точки са извън -> overfitting?
- Широка шапка -> всички точки са под нея
- Голяма $\gamma$ => голяма регуляризация.

kernal-а е функция, която ни дава разстояние.

Въпрос: каква е размерността на пространството на kernel-ите?
- линеен n -> n
- полиномен n -> 1 + n + (n choose 2)

Проблеми:
- не можем да интерпретираме, разберем резултатите
- много бавно


SVM е подходящ за:
- данни с много измерения (вкл ако променливите > записи). Дърветата и ансамблите - не са.
- неструктурирани данни
	- например текст


## k-nearest neighbours

k най-близки съседи

Избираме k, в примера долу е 5. Взимаме 5-те най-близки наблюдения. **Ефективният радиус** зависи къде избираме точката.

Предсказанието е:
- класификация -> избираме класа с най-голяма честота (**majority vote**)
- регресия -> средно или медиана

Имаме 100% accuracy на трениращите данни.
