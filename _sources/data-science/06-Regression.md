# 06 Regression

3 вида статистика: описателна (описваме какво виждаме от данните), прогнозна (предвиждане за бъдещето), предписателна (взимаме решение)

data analytics != data science
analytics: показване на метрики, 
science: feature selection / extraction / engineering, 

## Регресионни модели

всеки процес може да се опише като функция с вход и изход
схоластична = случайна

СЛУЧАЙНА ПРОМЕНЛИВА
случайна променлива = има случайни елементи, които и влияят
пример: разстоянието от забитата стреличка дартс до центъра на дъската
случайната пром няма фиксирани стойности
всяка стойност която мерим е всъщност разпределение, т.е. вероятности да е някакви интервали

правим проучване -> формулираме хипотеза  -> правим експеримент (контролираме входните данни) ->

МОДЕЛ
model = опростена версия на реалното
важно е да върши работа, да е достат добър
има предположения, има опростявания

Machine leaning = автоматизирано 

Модел != алгоритъм

интерполация != апроксимация
(задължително през точките) != (най-близо до разпределенията на всички точки)

точките на графиката не са фиксирани (error bars)

РЕГРЕСИЯ С/У КЛАСИФИКАЦИЯ
регресионни задачи и задачи за класификация
2те най-използвани класове задачи с който предсказваме
Р = предсказваме непрекъсната пром - колко ще е темп утре
К = предсказваме категория - дали на снимка е котка или не

както извадката показва за съвкупност, така модела показва за реалността

независими = входни данни
зависими = изходни данни

често между входните данни има зависимости

автокорелация = пример: корелация на променлива сама със себе си в time series между два последователни дни

периодичност, цикличност, повтаряемост също се нарича сезонност (не задължително периода да е сезон)

ще искаме да моделираме правата линия с нейното уравнение

данните са y, Модела е y_tilda (за да различим от y)
това е нашата хипотеза за изхода 
a,b = коеф които търсим

функция = Моделираща функция

модели на данните:



ортогонална регресия

за да сметне общото разстояние от точките до правата, компютърът ползва сивите отсечки, а не червените
причината: по-бързо е за смятане


### Errors

mean sq error MSE
error - защото е разлика между измерени и предсказани точки
mean - защото е средно аритметично
измисляме метрика за разстоянието Която ни е удобна да смятаме - в случея MSE
това е Loss фунцкията
МСЕ е индикация дали модела ни е добър, може да сравняваме 2 модела (2 линии)

квадратични разлики

total cost func
loss func

mean abs error

Минимизират функцията

## Градиентен метод

Градиентен метод = gradient descent

descent = слизане
gradient на функция = вектор от всички нейни частни първи производни
del = \partial

вектор = насочена отсечка
вектор = масив от числа


градиентът са стрелките долу

Градиентът сочи накъде расте най-много (посоката) и с колко расте (големината)

Г се държи като производна

ако искаме да видим накъде намаляваме най-бързо - 

ползваме Г за да:
	- за да открием кои параметри на cost фунцкията тя е в минимум, я минимизират
	- да намерим линия която е най-близо до измерените точки


Нp.random.seed(4242)
reproducibility
възпроизводимост на проучването - random генераторите ще дават един и също резултат при стартиране


reshape(-1,1)
-1 означава колкото дойде - толкова редове


Градиентният метод се ползва НАВСЯКЪДЕ освен при лин регресия

генерализиран линеен модел GLM

обличаме нормалната лин регресия в нова ф-я




