# 04 Trees and Ensembles


2 стратегии:

-- adding features - подходящо при таблични данни

-- keeping the data as raw as possible - подходящо при снимки, видео, аудио

Подходящ датасет за проекта:

50-80 columns

50k-100k

## Дървета за взимане на решения

Дървета за взимане на решения

Насочен ацикличен граф - DAG

Структура от данни "дърво"

Гора - комбинация от дървета

Tradeoff - компенсация

Impurity measure - функция, която може да извикаме за всяка част от данните

Метрика за безпорядкък

Impurity(node)

Impurity(child_yes), Impurity(child_no)

Най-добре разделяне имаме когато:

Impurity(node) >> Sum(impurity(childs))

От голямо разнообразно парче към възможно най-еднообразни парчета

Да ги разделим възможно най-адекватно

Impurity = I

Information gain = IG

IG = I_parent - sum(I_childs)

Ентропия - мярка за безпорядък

Не е парабола

Gini - друга метрика, по-бърза тъй като няма логаритъм

Pruning = окастряне на дървото; да намалим брой на листата с много малки записи

Например, задаваме стойност на хиперпараметъра min_sample_(per)leaf, min_samples_(per)_split

Вкарваме bias, но намалявам variance

Математически, регуляризация е само L1 и L2, но разговорно (тъй като и Pruning намалява) се включва

Нормата да се добави към cost функцията

В дърветата нямаме тегловни коеф (както при регресията)

Majority vote - взимаме класа с най-голяма пропорция

## Decision Forest (aka Random Forest)

дърветата оптимизират 

когато ограничим броя на въпросите, ограничаваме нестабилността

нито едно дърво в гората не вижда целия датасет

ако в ансамбъл (като гора), вместо дървета пуснем например регресия - **voting ensemble**

хомогенен ансамбъл

Горите са ансамблови алгоритми - очакваме да се справя по-добре и да не over-fit-ват, да се компенсират множеството дървета

Множество от дървета (ансамбъл)

Най-популярни алгоритми: 1. SVM 2. Random Forest

Няколко начина:

-- всяко дърво получава част от записите, всички колони

-- всяко дърво получава всички записи, но само от няколко колони

-- случайни колони и записи

Bagging - Много алгоритми, които получават извадка от данните (най-често част от записите), взимаме majority vote

Voting - даваме много различни алгоритъма

и им даваме тегла

Интерпретацията на резултатите при:

-- линейните алгоритми както и при 1 дърво е лесна - имаме тегловни коеф и въпроси (split)

-- горите е трудна . Една метрика е feature_importances_.

Параметър n_jobs - нишки на процесор, във всеки sklearn модел

Boosting  - букв. Подсилване

Decision stump - букв. дънер за взимане на решения

Често AdaBoost overfit-ва данните

RACSAC, AdaBoost, TrainTestSplit - все алгоритми, който помагат на други да си свършат работата ?

При AdaBoost не тунинговаме базовия estimator (алгоритъм), т.е. умишлено слагаме слаб алгоритъм

